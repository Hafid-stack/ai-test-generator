spring.application.name=ai-test-generator
# 1. Tell Spring Boot where Ollama is running on your Mac
spring.ai.ollama.base-url=http://localhost:11434

spring.ai.ollama.chat.options.model=llama3.1:8b

spring.ai.ollama.chat.options.temperature=0.2